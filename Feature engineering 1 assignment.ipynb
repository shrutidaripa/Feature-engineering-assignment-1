{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c456fa66",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING 1 ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e1b7e",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "ANS:-Missing values in a dataset refer to data points that are absent or unavailable for specific features (columns) in your data. These can occur due to various reasons like sensor malfunctions during data collection, human error while entering data, or participants skipping questions in a survey.\n",
    "\n",
    "Here's why handling missing values is essential:\n",
    "\n",
    "Biased Results: If you simply ignore missing values, it can bias your analysis. The remaining data might not be representative of the entire population, leading to inaccurate conclusions.\n",
    "Reduced Accuracy: Missing values can decrease the accuracy of many machine learning algorithms that rely on complete data for calculations.\n",
    "Modeling Issues: Some algorithms might not be able to function at all if there's a significant amount of missing data.\n",
    "However, there are a few algorithms that can handle missing values to some extent:\n",
    "\n",
    "K-Nearest Neighbors (KNN): This algorithm can estimate the missing value for a data point by considering the values of its nearest neighbors (data points with similar features) that have complete data for that specific feature.\n",
    "Decision Trees: These can handle missing values by following a specific strategy when encountering a missing value during the decision-making process. For example, it might consider the most common value for that feature or follow a pre-defined rule for handling missing data.\n",
    "Naive Bayes: This algorithm assumes independence between features, so a missing value in one feature doesn't necessarily affect the calculation for another feature. However, the presence of missing values can still impact the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35382e9b",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "ANS:-1. Deletion:\n",
    "\n",
    "This is a simple approach where you remove rows or columns with missing values. However, this can be wasteful if you have a large dataset or a small amount of missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    C\n",
      "0  10\n",
      "1  11\n",
      "2  12\n",
      "3  13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {'A': [1, 2, np.nan, 5], 'B': [6, np.nan, 8, 9], 'C': [10, 11, 12, 13]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values (axis=0 for rows)\n",
    "df_dropna = df.dropna(axis=0)\n",
    "\n",
    "# Drop columns with missing values (axis=1 for columns)\n",
    "df_dropna = df.dropna(axis=1)\n",
    "\n",
    "print(df_dropna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedecf9",
   "metadata": {},
   "source": [
    "2. Imputation:\n",
    "\n",
    "This involves replacing missing values with estimated values. Here are two common imputation techniques:\n",
    "a) Mean/Median Imputation:\n",
    "\n",
    "Replace missing values with the mean (for numerical features) or median (for categorical features) of the entire column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa3bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B   C\n",
      "0  1.000000  6.000000  10\n",
      "1  2.000000  2.666667  11\n",
      "2  2.666667  8.000000  12\n",
      "3  5.000000  9.000000  13\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean (replace with median for categorical data)\n",
    "df_imputed = df.fillna(df['A'].mean())  # Fill with mean of column A\n",
    "\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5890f",
   "metadata": {},
   "source": [
    "b) Forward Fill/Backward Fill:\n",
    "\n",
    "Replace missing values with the value from the previous (forward fill) or next (backward fill) non-missing data point in the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf0a347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  6.0  10\n",
      "1  2.0  6.0  11\n",
      "2  2.0  8.0  12\n",
      "3  5.0  9.0  13\n",
      "     A    B   C\n",
      "0  1.0  6.0  10\n",
      "1  2.0  8.0  11\n",
      "2  5.0  8.0  12\n",
      "3  5.0  9.0  13\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with forward fill\n",
    "df_forward_fill = df.fillna(method='ffill')\n",
    "\n",
    "# Impute missing values with backward fill\n",
    "df_backward_fill = df.fillna(method='bfill')\n",
    "\n",
    "print(df_forward_fill)\n",
    "print(df_backward_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6403ed",
   "metadata": {},
   "source": [
    "3. Interpolation:\n",
    "\n",
    "This technique is for numerical features and involves estimating missing values based on surrounding data points. Common methods include linear interpolation, spline interpolation, etc. Libraries like scipy.interpolate can be used for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015bb2a",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "ANS:-Imbalanced data refers to a situation in machine learning where a dataset has a significant difference in the number of examples for different classes. Typically, one class (the majority class) has many more data points than other classes (the minority class). This imbalance can pose challenges for machine learning models.\n",
    "\n",
    "Consequences of Not Handling Imbalanced Data:\n",
    "\n",
    "Here's what can happen if you don't address imbalanced data in your machine learning model:\n",
    "\n",
    "Biased Models: Machine learning algorithms often optimize for overall accuracy. In imbalanced data, a model can achieve high accuracy by simply predicting the majority class for all instances. This overlooks the minority class and fails to capture the patterns specific to it.\n",
    "Poor Performance on Minority Class: The model might perform well on the majority class but poorly on the minority class. This can be problematic in situations where accurate predictions for the minority class are crucial, such as fraud detection or medical diagnosis of rare diseases.\n",
    "Unreliable Evaluation Metrics: Metrics like accuracy become unreliable when dealing with imbalanced data. A high accuracy might not reflect the model's ability to handle the minority class effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be130ec",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "ANS:-Up-sampling and down-sampling are techniques used to address imbalanced data in machine learning, where one class has significantly more examples than others. They work by manipulating the class distribution of your data to create a more balanced dataset for training your model.\n",
    "\n",
    "1. Up-sampling:\n",
    "\n",
    "Concept: This technique increases the number of data points in the minority class. There are two main approaches:\n",
    "\n",
    "Replication: Simply duplicate existing data points from the minority class. This is a straightforward method but can lead to overfitting as the model learns the replicated examples too well.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This method creates synthetic data points for the minority class. It identifies existing data points in the minority class and creates new points along the line segments between them, essentially interpolating new examples within the existing feature space of the minority class.\n",
    "\n",
    "When to Use: Up-sampling is suitable when you have a very limited amount of data for the minority class and want to maximize the information you can learn from it. However, be cautious of overfitting if using simple replication.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "Concept: This technique reduces the number of data points in the majority class. It's a simpler approach but can discard potentially valuable information.\n",
    "\n",
    "When to Use: Down-sampling is appropriate when you have a large dataset and computational resources might be limited. It can also be helpful if you suspect the majority class data might be noisy or contain outliers that could skew the model's learning.\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine you're training a model to classify emails as spam or not spam. Your dataset might have 99% non-spam emails and only 1% spam emails.\n",
    "\n",
    "Up-sampling: You could duplicate existing spam emails or use SMOTE to create synthetic spam emails to balance the class distribution.\n",
    "\n",
    "Down-sampling: You could randomly remove non-spam emails from the dataset until the number of non-spam emails is closer to the number of spam emails.\n",
    "\n",
    "Choosing the Right Technique:\n",
    "\n",
    "The best approach (up-sampling or down-sampling) depends on the specific characteristics of your data and the available computational resources. Here are some general considerations:\n",
    "\n",
    "Data Availability: If you have very limited data, up-sampling might be necessary to get enough information for the minority class.\n",
    "Overfitting Risk: Up-sampling, especially with simple replication, can lead to overfitting. Consider using SMOTE or down-sampling if overfitting is a concern.\n",
    "Data Quality: If the majority class data is noisy, down-sampling can help remove that noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafe25e",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "ANS:-Data Augmentation and SMOTE: Techniques for Imbalanced Learning\n",
    "Data augmentation and SMOTE (Synthetic Minority Over-sampling Technique) are both techniques used in machine learning to address the challenge of imbalanced datasets. Imbalanced data occurs when a dataset has a significant difference in the number of examples for different classes, with one class (the majority class) having many more data points than others (the minority class). This imbalance can lead to models that are biased towards the majority class and perform poorly on the minority class.\n",
    "\n",
    "Data Augmentation:\n",
    "\n",
    "Concept: Data augmentation is a broader approach that involves artificially creating new training data points from existing ones. This can be done through various transformations, such as:\n",
    "\n",
    "Image data: Flipping images horizontally, rotating them, adding noise, or applying color jittering.\n",
    "Text data: Synonym replacement, random deletion/insertion of words, or paraphrasing sentences.\n",
    "Time series data: Shifting the time series, scaling the data, or adding noise.\n",
    "Purpose: The goal of data augmentation is to increase the diversity of the training data and improve the model's ability to generalize to unseen examples. It can also help prevent overfitting by reducing the model's reliance on any specific data points.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "Concept: SMOTE is a specific data augmentation technique designed specifically for imbalanced datasets. It focuses on oversampling the minority class by creating synthetic data points.\n",
    "\n",
    "Process:\n",
    "\n",
    "SMOTE identifies a data point in the minority class.\n",
    "It finds its k nearest neighbors (other data points in the minority class that are similar based on features).\n",
    "SMOTE randomly selects one of these k neighbors.\n",
    "It creates a new synthetic data point by taking a random point along the line segment between the original data point and its selected neighbor.\n",
    "Benefits: SMOTE helps to balance the class distribution by increasing the number of data points in the minority class. This can lead to models that are more aware of the minority class and perform better on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e537b5",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "ANS:-In machine learning, outliers are data points that fall significantly outside the typical range of values for the other data points in a dataset. They can be much larger (positive outliers) or much smaller (negative outliers) compared to the rest of the data.\n",
    "\n",
    "Here's why it's essential to handle outliers:\n",
    "\n",
    "1. Impact on Statistical Analysis: Outliers can significantly skew the results of statistical measures like mean, median, and standard deviation. These measures are often used to summarize and understand the data, but outliers can distort the true central tendency and spread of the data.\n",
    "\n",
    "2. Biasing Machine Learning Models: Many machine learning algorithms rely on assumptions about the underlying distribution of the data. Outliers can violate these assumptions and lead to biased models that perform poorly on unseen data. For instance, an outlier with an extremely high value might disproportionately influence the model's predictions for other data points.\n",
    "\n",
    "3. Masking Underlying Patterns: Outliers can sometimes mask important relationships or patterns within the data. If you simply ignore them, you might miss valuable insights that could be crucial for your analysis or model building.\n",
    "\n",
    "4. Model Sensitivity: Some machine learning algorithms, particularly those based on distance metrics (like K-Nearest Neighbors), can be very sensitive to outliers. These outliers can pull the model's decisions in unexpected directions, leading to inaccurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1062089",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "ANS:-Customer data analysis can be hampered by missing information. Luckily, there are several techniques to address this:\n",
    "\n",
    "Deletion: This involves removing rows with missing values (listwise deletion) or columns with a high percentage of missing values. It's simple but can discard valuable data, especially if missingness is widespread.\n",
    "\n",
    "Imputation: This replaces missing values with estimates. There are various methods:\n",
    "\n",
    "Mean/Median/Mode: Replace with the average, middle, or most frequent value for numerical data (be cautious of bias).\n",
    "Random Sample Imputation: Fill with a random value from existing data (may not reflect underlying patterns).\n",
    "K-Nearest Neighbors (KNN): Estimate missing values based on similar data points (effective but requires choosing the right number of neighbors 'K').\n",
    "Model-based Imputation: Use machine learning models to predict missing values based on other variables (sophisticated but requires building and validating models).\n",
    "Weighting: Assign weights to data points based on completeness, giving more importance to rows with fewer missing values.\n",
    "\n",
    "Selection Models: Use techniques like regression or decision trees to create a model that predicts missing values based on other variables (useful when missingness is related to specific factors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f95a3",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "ANS:-When dealing with missing data in a large dataset, especially a small percentage, here are some strategies to determine if it's missing at random (MCAR) or not:\n",
    "\n",
    "Comparison of Groups:\n",
    "Split the data into groups based on observed variables (e.g., income brackets, geographic regions).\n",
    "Compare the distribution of missing values across these groups.\n",
    "If a particular group has a significantly higher rate of missingness for a specific variable, it suggests the missingness is not random (Missing At Random - MAR or Missing Not At Random - MNAR).\n",
    "Little's MCAR Test:\n",
    "This statistical test specifically checks for Missing Completely At Random (MCAR) data.\n",
    "It compares the complete cases (observations with no missing values) to the incomplete cases (observations with at least one missing value) on all available variables.\n",
    "If there's no statistically significant difference between the groups, it lends credence to the data being MCAR.\n",
    "Examine Missingness Patterns:\n",
    "Look for patterns in how data is missing.\n",
    "Are there entire rows or columns missing?\n",
    "Are missing values clustered in specific areas?\n",
    "Random missingness would exhibit a scattered pattern across the data.\n",
    "Correlations with Missing Indicator:\n",
    "Create a new binary variable indicating missing values (1 for missing, 0 for observed).\n",
    "Calculate correlations between this variable and other observed variables.\n",
    "Significant correlations suggest the missingness is related to the observed data (MAR or MNAR), not random.\n",
    "Domain Knowledge:\n",
    "Consider the data collection process and potential reasons for missing values.\n",
    "For example, if income data is missing for low-income respondents who might be hesitant to disclose it, the missingness wouldn't be random (MNAR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c964a4",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "ANS:-In medical diagnosis, where the target condition might be rare, you're dealing with a classic imbalanced dataset. Here are key strategies to evaluate your machine learning model's performance:\n",
    "\n",
    "Choose Appropriate Metrics:\n",
    "Accuracy: Though common, it's misleading for imbalanced data. A high accuracy might simply reflect the model's ability to predict the majority class (patients without the condition).\n",
    "\n",
    "Focus on metrics sensitive to the minority class:\n",
    "\n",
    "Recall (Sensitivity): Measures the model's ability to correctly identify patients with the condition (true positives). A low recall indicates the model misses many cases of the disease.\n",
    "Precision (Positive Predictive Value): Measures the proportion of positive predictions that are actually true positives. A low precision suggests the model incorrectly flags many healthy patients as having the condition.\n",
    "F1-Score: Combines precision and recall into a single metric, providing a balanced view of the model's performance for both classes.\n",
    "\n",
    "Visualization Techniques:\n",
    "Confusion Matrix:  A table visualizing how many data points were correctly or incorrectly classified for each class. It helps identify issues like high false negatives (missed disease cases).\n",
    "\n",
    "ROC Curve (Receiver Operating Characteristic Curve):  Plots the True Positive Rate (recall) against the False Positive Rate (1 - specificity) for various classification thresholds. It allows comparison of different models and helps assess the trade-off between correctly identifying the disease and minimizing false alarms.\n",
    "\n",
    "Cost-Sensitive Learning:\n",
    "Assign different costs to misclassifications. In medical diagnosis, a false negative (missing a disease case) might be more costly than a false positive (unnecessary test on a healthy patient). The model can be trained to optimize for these costs.\n",
    "Comparison with Baseline Models:\n",
    "Compare your model's performance to simpler baselines, like always predicting the majority class (no disease). This highlights the added value of your model in identifying the rare condition.\n",
    "Data Balancing Techniques (if applicable):\n",
    "Depending on the scenario and data availability, consider techniques like oversampling (replicating data points from the minority class) or undersampling (reducing data points from the majority class) to create a more balanced dataset for training. However, these techniques require careful application to avoid introducing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c88ae",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "ANS:-When estimating customer satisfaction in an imbalanced dataset with mostly satisfied customers, here are methods to balance the data through down-sampling the majority class:\n",
    "\n",
    "1. Random Down-Sampling:\n",
    "\n",
    "This is the simplest approach. You randomly remove entries from the \"satisfied\" class until it has a similar size to the \"dissatisfied\" class. This ensures both classes have roughly equal representation during model training.\n",
    "\n",
    "2. Stratified Down-Sampling:\n",
    "\n",
    "This refines random down-sampling by maintaining the original class proportions within the \"satisfied\" class. You can divide satisfied customers into subgroups based on relevant factors (e.g., product type, purchase region) and then randomly remove entries from each subgroup proportionally. This helps preserve the diversity of the satisfied class.\n",
    "\n",
    "3. Reservoir Sampling:\n",
    "\n",
    "This is a memory-efficient down-sampling technique suitable for large datasets. It iterates through the \"satisfied\" class, keeping a fixed-size reservoir to hold potential data points. With a certain probability, it replaces entries in the reservoir with new ones. By carefully adjusting the probability, you can achieve the desired down-sampling ratio.\n",
    "\n",
    "Important Considerations for Down-Sampling:\n",
    "\n",
    "Information Loss: Down-sampling reduces the overall data size, potentially discarding valuable information from the majority class.\n",
    "Balance Ratio: Choosing the right ratio for down-sampling is crucial. A balanced dataset doesn't necessarily mean equal class sizes. It depends on the specific problem and the cost of misclassification.\n",
    "Alternatives to Down-Sampling: Depending on the scenario, explore up-sampling techniques (replicating data points from the minority class) or using cost-sensitive learning algorithms that assign higher weights to the minority class during training.\n",
    "Additional Tips:\n",
    "\n",
    "Evaluate Model Performance: After balancing, assess your model's performance using metrics like precision, recall, and F1-score, paying close attention to how it handles the minority class (dissatisfied customers).\n",
    "Domain Knowledge: Consider incorporating domain knowledge about customer satisfaction. For instance, you might prioritize including reviews with specific keywords indicating strong dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827dc35",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "ANS:-When working with a rare event in an imbalanced dataset, up-sampling the minority class can be a useful strategy. Here are methods to achieve this:\n",
    "\n",
    "1. Random Over-Sampling:\n",
    "\n",
    "This is the simplest approach. You randomly duplicate data points from the minority class (rare event occurrences) until it reaches a similar size or a desired proportion compared to the majority class. This increases the model's exposure to the rare event and helps it learn the patterns better.\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-Sampling Technique):\n",
    "\n",
    "This is a more sophisticated technique that creates synthetic data points for the minority class. It identifies existing minority class data points and generates new data points along the line segments connecting them to their nearest neighbors in the feature space. This approach helps address the potential issue of overfitting that can occur with simple oversampling, where just replicating existing data might not capture the full range of variations within the rare event.\n",
    "\n",
    "3. ADASYN (Adaptive Synthetic Minority Over-Sampling Technique):\n",
    "\n",
    "This is an advanced version of SMOTE that takes into account the difficulty of learning each minority class example.  ADASYN assigns higher weights to \"harder-to-learn\" examples (those with a higher density of majority class points around them) during the synthetic data generation process.  This focuses the oversampling on the more informative minority class data points, potentially leading to better model performance.\n",
    "\n",
    "Important Considerations for Up-Sampling:\n",
    "\n",
    "Overfitting: Oversampling increases the data size, but it can also lead to the model overfitting to the replicated data, especially with random oversampling. Techniques like SMOTE can mitigate this issue.\n",
    "Class Imbalance Ratio: The chosen up-sampling ratio needs to be carefully considered. Oversampling the minority class too much can overshadow the majority class and lead to biased models.\n",
    "Alternatives to Up-Sampling: Depending on the scenario, explore down-sampling the majority class or using cost-sensitive learning algorithms that assign higher weights to the minority class during training.\n",
    "Additional Tips:\n",
    "\n",
    "Data Cleaning and Validation: Before up-sampling, ensure the minority class data is clean and free of errors. Up-sampling noisy data can amplify the noise and mislead the model.\n",
    "Evaluation Metrics: When evaluating your model's performance, use metrics sensitive to the minority class, such as precision, recall, and F1-score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
